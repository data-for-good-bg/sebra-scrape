{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "https://www.minfin.bg/bg/transparency/ <br>\n",
    "\n",
    "- currently does not download MF_; NF files -> search for \"@change files\" to change\n",
    "\n",
    "\n",
    "to do:\n",
    "time.sleep change, double check scrapy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "#!{sys.executable} -m pip install scrapy\n",
    "\n",
    "from datetime import date,datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already existing directory:  /Users/bilyanavencheva/Desktop/All/DG_test/SEBRA\n",
      "Path where files will be downloaded:  /Users/bilyanavencheva/Desktop/All/DG_test\n",
      "Start date selected: 2021-05-15\n",
      "End date selected: 2021-05-27\n",
      "All relevant urls to be crawled for the specified period exported\n"
     ]
    }
   ],
   "source": [
    "path = r'/Users/bilyanavencheva/Desktop/All/DG_test' #r'/Users/bilyanavencheva/Desktop/All/DataForGood'\n",
    "\n",
    "## create sub directories to store the different types of documents\n",
    "# @change files\n",
    "folders = ['/SEBRA']#['/SEBRA','/NF_SEBRA','/MF_SEBRA']\n",
    "\n",
    "#where to start: start and end date are defined later (if data is already available in folders, below values are not used)\n",
    "min_year = 2021\n",
    "min_month = 5\n",
    "min_day = 15\n",
    "\n",
    "def get_urls(min_year = min_year, min_month = min_month, min_day = min_day):\n",
    "    #max date always current date\n",
    "    max_year = datetime.now().date().year\n",
    "    years = [str(year) for year in range(min_year,max_year + 1)]\n",
    "\n",
    "    for folder in folders:\n",
    "        #if folder already exists, check if there are subfolders with years, create if necessary\n",
    "        if os.path.exists(path + folder) == True:\n",
    "            print (\"Already existing directory: \", path + folder)\n",
    "            for y in years:\n",
    "                try:\n",
    "                    os.mkdir(path + folder + '/' + y)\n",
    "                except FileExistsError:\n",
    "                    pass\n",
    "        else:\n",
    "            try:\n",
    "                os.mkdir(path + folder)\n",
    "                print (\"Successfully created the directory: \", path + folder)\n",
    "            except OSError:\n",
    "                print (\"Failure creation of the directory:\", path + folder)\n",
    "\n",
    "            for y in years:\n",
    "                try:\n",
    "                    os.mkdir(path + folder + '/' + y)\n",
    "                except FileExistsError:\n",
    "                    pass\n",
    "\n",
    "    #get latest SEBRA date imported\n",
    "\n",
    "    imported_dates = []\n",
    "    for year in years:\n",
    "        try:\n",
    "            max_date = max([datetime.strptime(date[:10],'%Y-%m-%d').date() for date in os.listdir(path + '/SEBRA/' + year) if type(re.search('\\d{4}-\\d{2}-',date)) == re.Match])\n",
    "            imported_dates.append(max_date)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    # if there are already imported files, re-assign     \n",
    "    if len(imported_dates) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        max_imported_date = max(imported_dates)\n",
    "        min_day = max_imported_date.day\n",
    "        min_month = max_imported_date.month\n",
    "        min_year = max_imported_date.year\n",
    "\n",
    "    #starting date to search for data\n",
    "    start_date = date(min_year,min_month, min_day) #2011-07-22\n",
    "    end_date = datetime.now().date() #date(2021, 12, 31)\n",
    "    \n",
    "    assert start_date <= end_date,'Start date {} is not before end date {}'.format(start_date,end_date)\n",
    "\n",
    "    print('Path where files will be downloaded: ', path)\n",
    "    print('Start date selected: {}'.format(start_date))\n",
    "    print('End date selected: {}'.format(end_date))\n",
    "    \n",
    "    #defines the date range: weekends are not included (normally no data is expected here)\n",
    "    dates =[str(date.date()) for date in pd.date_range(start  = start_date, end = end_date, periods = None, freq='B')]\n",
    "\n",
    "    ## extract the urls with the relevant dates\n",
    "    urls = {}\n",
    "    for d in dates:\n",
    "        url = 'https://www.minfin.bg/bg/transparency/' + d\n",
    "        urls[d] = url\n",
    "    print('All relevant urls to be crawled for the specified period exported')\n",
    "    \n",
    "    return urls,dates,start_date,end_date\n",
    "\n",
    "urls,dates,start_date,end_date = get_urls()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of dates:  9\n",
      "Total number of files (incl NF and MF) available:  18\n",
      "Total number of pages checked:  9\n",
      "Total number of dates with <> 3 files 3\n",
      "No issues with url request\n",
      "No issues with download of files\n",
      "No issues with renaming of files\n",
      "Meta data saved in pickled file: dates_meta_2021-05-15_2021-05-27\n",
      "CPU times: user 441 ms, sys: 160 ms, total: 601 ms\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "issues_requests = {}\n",
    "issues_download = {}\n",
    "issues_rename = {}\n",
    "soups = {}\n",
    "processed_files = 0\n",
    "\n",
    "with requests.Session() as s:\n",
    "    for d in urls.keys():\n",
    "        \n",
    "        \n",
    "        year = d[:4]\n",
    "        html = s.get(urls[d])\n",
    "\n",
    "        if html.status_code != 200:\n",
    "            issues_requests[d] = 'Status request: ' + str(html.status_code)\n",
    "        else:\n",
    "            soups[d] = {'num_files':0,'num_upload':0,'files':[]}\n",
    "            soup = BeautifulSoup(html.content,'html.parser')\n",
    "\n",
    "            num_files = 0\n",
    "            num_upload = 0\n",
    "            files = []\n",
    "\n",
    "            for link in soup.find_all('a'):\n",
    "                #checks and counts if upload is mentioned in the html\n",
    "                if 'upload' in link.get('href'):\n",
    "                    num_upload += 1 \n",
    "                #checks and counts if .xl is mentioned in the html, extracts the links !! to do might need to look for other extensions\n",
    "                if '.xl' in link.get('href'):\n",
    "                    num_files += 1\n",
    "                    files.append('https://www.minfin.bg' + link.get('href'))\n",
    "\n",
    "            if len(files) > 0:  #if we have any files downloaded\n",
    "                # check whether the file already exists -> if it does -> replace with new file\n",
    "                for file in files:\n",
    "                    \n",
    "                    # @change files\n",
    "                    if 'MF' in file or 'NF' in file:  #this line needs to be dropped if we want to download all of them\n",
    "                        continue   #this line needs to be dropped if we want to download all of them\n",
    "                    else:  #this line needs to be dropped if we want to download all of them\n",
    "\n",
    "                        processed_files =+ 1\n",
    "                        url = file\n",
    "                        if \"NF\" in file:\n",
    "                                location  = path + folders[1] + '/' + year\n",
    "                        elif \"MF\" in file:\n",
    "                                location  = path + folders[2] + '/' + year\n",
    "                        else:  \n",
    "                                location  = path + folders[0] + '/' + year\n",
    "\n",
    "                        file_name = [r.end() for r in re.finditer('/',file)][-1] - 1\n",
    "\n",
    "                        #if os.path.exists(location + file[last:]) == True:\n",
    "                            #continue\n",
    "                        #else:\n",
    "                        try:\n",
    "                            # dir(webdriver.common.html5.application_cache.ApplicationCache)\n",
    "                            customOption = Options()\n",
    "                            customOption.add_experimental_option('prefs',{'download.default_directory':location})\n",
    "                            customOption.add_argument(\"--headless\")\n",
    "                            driver = webdriver.Chrome(executable_path = r'/users/bilyanavencheva/Applications/chromedriver', options = customOption)\n",
    "                            driver.get(url)\n",
    "                            time.sleep(5) ## this needs to be addressed\n",
    "                            driver.close()\n",
    "                        except Exception as e:\n",
    "                            issues_download[file] = e\n",
    "\n",
    "                        ##normalize the date\n",
    "                        try:\n",
    "                            os.rename(location + '/'+ file.split('/')[-1],location + '/'+ d + '_' + file.split('/')[-1])\n",
    "                        except Exception as e:\n",
    "                            issues_rename[file] = e\n",
    "            else: \n",
    "                continue\n",
    "\n",
    "            soups[d]['num_files']  = num_files\n",
    "            soups[d]['num_upload'] = num_upload\n",
    "            soups[d]['files'] = files\n",
    "\n",
    "#add new meta data to previous one            \n",
    "dates_meta = pd.DataFrame.from_dict(soups,orient ='index') #pd.concat([dates_meta,pd.DataFrame.from_dict(soups,orient ='index')])\n",
    "\n",
    "# files_in_folders = len([file for file in os.listdir(path + folders[0] + '/' + year) if '.xl' in file ]) \\\n",
    "#     + len([file for file in os.listdir(path + folders[1] + '/' + year) if '.xl' in file ]) \\\n",
    "#     + len([file for file in os.listdir(path + folders[2] + '/' + year) if '.xl' in file ])\n",
    "\n",
    "#summary messages & checks\n",
    "print('Total number of dates: ', len(dates))\n",
    "print('Total number of files (incl NF and MF) available: ', sum(dates_meta['num_files']))\n",
    "\n",
    "#print('Total number of files processed: ', processed_files)\n",
    "print('Total number of pages checked: ', len(urls))\n",
    "print('Total number of dates with <> 3 files', len(dates_meta[dates_meta.num_files != 3]))\n",
    "\n",
    "#requests\n",
    "if len(issues_requests) == 0:\n",
    "    print('No issues with url request')\n",
    "else:\n",
    "    print('Number of issues url requests: ', len(issues_requests))\n",
    "\n",
    "#downloads    \n",
    "if len(issues_download) == 0:\n",
    "    print('No issues with download of files')\n",
    "else:\n",
    "    print('Number of issues with download of files: ', len(issues_download))\n",
    "    \n",
    "#rename    \n",
    "if len(issues_rename) == 0:\n",
    "    print('No issues with renaming of files')\n",
    "else:\n",
    "    print('Number of issues with renaming of files: ', len(issues_rename))\n",
    "\n",
    "# save pickle file   \n",
    "name = 'dates_meta_' + str(start_date) + \"_\" + str(end_date)\n",
    "with open(path + '/'+ name, 'wb') as config_dictionary_file:\n",
    "    pickle.dump(dates_meta, config_dictionary_file)\n",
    "\n",
    "print('Meta data saved in pickled file:',name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
